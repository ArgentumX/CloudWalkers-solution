# Система
- оперативная память: 48G
- python: 3.9

# Предобработка данных: 
мы ограничились нормализацией id в некоторых моделях, отбрасыванием части фичей датасета. Прочие испробованные методы не приносили существенной пользы.

# Решение:
для сортировки фичей по важности используется встроенный функционал catboost - модели, обученной на 501(+id) параметрах.

HistGradientBoostingClassifier из sklearn, обученный на 501 фичах - перввая наша модель, преодолевшая порог roc-auc 0.85. 
Были попытки с нейронными сетями на pytorch, tensorflow, и они имели определённый успех, однако выше 0.855 их roc-auc мы поднять не смогли, так что отказались от них в дальнейшем.

catboost и lightautoml - показали наилучшие результаты, мы сосредоточились на подборе оптимальных для них параметров и достигли некоторого успеха.
в финальном решении используются предсказания 9 моделей, из которых 6 - catboost, 3 - lightautoml. Такая конфигурация показала себя наилучшей. [обучение этих моделей ](https://github.com/ArgentumX/CloudWalkers-solution/blob/main/Models.ipynb)

Здесь собраны модели, с разным количеством отброшенных фич(0, 300, 350, 400), что скорее всего является основной причиной высокой метрики при их дальнейшем блендинге. 
Можно предположить, что при разном количестве фич модели выделяют закономерности на разных слоях, то есть, к примеру в (отсортировано по важности) фичах 200-300 содержится некоторая закономерность, связанная с результатом, а в фичах 100-200 есть своя закономерность.
При обучении на всех фичах, модель может не выделить что-то важное.

далее мы производим самый простой (усреднённое значение) блендинг предсказаний и получаем финальное предсказание. [блендинг](https://github.com/ArgentumX/CloudWalkers-solution/blob/main/Blending.ipynb)

Вероятно, метрику можно увеличить ещё больше, если правильно изменить коэффиценты при блендинге, но наши попытки их автоматического поиска не принесли положительных результатов (в том числе из-за нехватки времени), так что мы остановились на усреднение предссказаний.
